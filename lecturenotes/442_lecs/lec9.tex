\section{Thursday, March 6: Fitting a neural network: backpropagation and gradient descent}

Given our neural network architecture and our training data, we need a way to FIT the network. This means coming up with values for all of the $w$s and all of the $\beta$s. These should be chosen to give us good predictive accuracy on the training set (although we also want to be careful of overfitting). We will fit a neural network with gradient descent (a very general technique). They key insight for neural networks is that we can use the chain rule to do gradient descent in a very organized way, called backpropagation. 

\subsection{Gradient descent}

Let $\theta$ be a really long vector that denotes our current set of parameters (values for every line in our neural network picture). Recall that $\theta$ consists of $w$s and $\beta$s, where $w$s operate between the inputs and the hidden layers, and $\beta$s act on the activation layer to give us the output. 

Given $\theta$, we can send an input example $X_i$ ``forward" through the network. This gives us a prediction $\hat{Y}_i$. 

Since this is a training example, we can then compute the loss: $(Y_i - \hat{Y}_i)^2$. 

If we do this over the whole training set, we get the overall loss for this current value of $\theta$:
$$
L(\theta) = \sum_{i=1}^n (Y_i - \hat{Y}_i)^2.
$$

Whenever we have a loss function that we want to optimize with respect to $\theta$, we wish we could take the derivative and set it equal to $0$. Here, there is no way we will be able to solve for that in closed form!

Instead, let's take steps along the negative gradient (with respect to $\theta$) of the loss function. The gradient of $L(\theta)$ with respect to $\theta$ is denoted $\nabla L(\cdot)$. The gradient is simply the vector of partial derivatives with respect to each element of $\theta$.  

We want to take steps in the direction of the negative gradient of the loss function so that we always move a current guess $\theta^t$ to a new guess $\theta^{t+1}$ in a way that will make the loss smaller the next time we compute it. What a nice idea!

While we take a step in the ``direction" of the gradient, we don't want to take a step that is the SIZE of the gradient. We want to take really small steps so that we don't ``overshoot" a minimum. So we let: 
$$
\theta^{t+1} = \theta^t - \rho \nabla L(\theta^{t}),
$$
where $\rho$ is a small learning rate that we get to pick.

Note: if our step size $\rho$ is going to be approximately the same for all elements of $\theta$, we probably want the elements of $\theta$ to all be in the same ``units". This means that we probably want to normalize our input variables $X$! This is not changed from other algorithms. 

We keep taking small steps in the direction of the negative gradient until our ``guesses" stop changing. At this point, we have landed in a region of $\theta$-space where the derivative is 0! This hopefully means that we found a minimum of the loss function!

Unfortunately, there is no guarantee that it is a global minimum. For really complicated non-convex loss functions, we can get stuck in suboptimal local minima. We will discuss strategies for avoid it later! For now, see Figure~\ref{fig_gd} for an illustration of gradient descent in a setting where $\theta$ is one-dimensional. 

Gradient descent is a really general optimization technique! It is used in contexts that have nothing to do with fitting neural networks!

\begin{figure}[h]
\centering 
\includegraphics[width=0.7\textwidth]{442_lecs/gradient.png}
\caption{An illustration from ISL of gradient descent for a very simple one-dimensional $\theta$. In this case, we found the global minimum. But that is not guaranteed!}
\label{fig_gd}
\end{figure}

\subsection{Backpropagation and the Chain Rule}

Let's study gradient descent for a one-layer feed-forward neural network in the following very simple case. This is a network with one hidden later and three hidden nodes ($K=3)$. There are also only three inputs ($p=3$). For simplicity, I did not draw the intercept (bias) terms only the diagram.

\begin{center}
\begin{tikzpicture}[
    node distance=1.5cm and 2.5cm,
    neuron/.style={circle, draw, minimum size=1cm},
    every label/.style={font=\small}
]

% Input layer
\node[neuron, fill=yellow!50] (I1) {$x_1$};
\node[neuron, fill=yellow!50, below=of I1] (I2) {$x_2$};
\node[neuron, fill=yellow!50, below=of I2] (I3) {$x_3$};

% Hidden layer
\node[neuron, fill=cyan!30, right=of I1, label=above:\textcolor{blue}{$A_1$}] (H1) {$g(w_1^T x)$};
\node[neuron, fill=cyan!30, right=of I2, label=above:\textcolor{blue}{$A_2$}] (H2) {$g(w_2^T x)$};
\node[neuron, fill=cyan!30, right=of I3, label=above:\textcolor{blue}{$A_3$}] (H3) {$g(w_3^T x)$};

% Output layer
\node[neuron, fill=purple!50, right=of H2, label=above:\textcolor{purple}{$\hat{y}$}] (O) {$f(\beta_0 + \beta^T A)$};

% Connections (Input to Hidden)
\foreach \i in {I1, I2, I3}
    \foreach \h in {H1, H2, H3}
        \draw[->] (\i) -- (\h);

% Connections (Hidden to Output)
\foreach \h in {H1, H2, H3}
    \draw[->] (\h) -- (O);

% Label the specific weights
\draw[->] (I1) -- node[above] {\small $w_{11}$} (H1);
\draw[->] (I3) -- node[above] {\small $w_{31}$} (H3);
\draw[->] (H1) -- node[above] {\small $\beta_1$} (O);
\draw[->] (H2) -- node[above] {\small $\beta_2$} (O);
\draw[->] (H3) -- node[above] {\small $\beta_3$} (O);
\end{tikzpicture}
\end{center}

In this case, suppose that we are doing regression, and so the function $f()$ at the very end is just the identity function. Recall that when we are doing classification, we need to turn our final output into a probability or a prediction using a link function. But for regression we are all set on our original scale. 

For given values of $w_{10},\ldots, w_{pK}$ and $\beta_0,\ldots, \beta_K$, each input example $x_i$ gets turned into a $\hat{y}_i$. Our squared error loss for our training set is: 
\begin{align*}
L(\theta) &= \sum_{i=1}^n L_i(\theta) = \sum_{i=1}^n (y_i - \hat{y}_i)^2 \\
&= \sum_{i=1}^n \left( y_i - \beta_0 - \sum_{k=1}^3 \beta_k A_k \right)^2 \\
&= \sum_{i=1}^n \left( y_i - \beta_0 - \sum_{k=1}^3 \beta_k g \left(w_{k0} + \sum_{j=1}^p w_{kj} x_{ij}\right)\right)^2. 
\end{align*}

I wrote this at multiple levels of granularity. Sometimes it is nice to explicitly see that the loss function depends on every $w$ and every $\beta$. Other times, it is kind of nice to hide this detail. 

Suppose that we begin with random guesses for every $\beta$ and every $w$. Then, we send our entire training set ``forward" through the neural network and compute this loss function given the current predictions. Then, we compute the gradient. We step along the gradient, and we update our guesses of $w$ and $\beta$ accordingly. 

For our neural network loss function, let's figure out the steps along the gradient. The key idea is that we can chain rule! 

In this example, since $f()$ is just the identity function, we have that:
\begin{align*}
\frac{dL}{d \beta_k} &= -2 \sum_{i=1}^n (y_i - \hat{y}_i) \frac{d\hat{y}_i}{d \beta_k} \\
&= - 2 \sum_{i=1}^n (y_i - \hat{y}_i) A_k \\
&= -2 \sum_{i=1}^n (y_i - \hat{y}_i) g \left(w_{k0} + \sum_{j=1}^p w_{kj} x_{ij}\right) 
\end{align*}
And then:
\begin{align*}
\frac{dL}{d w_{kj}} &= -2 \sum_{i=1}^n (y_i - \hat{y}_i) \frac{d\hat{y}_i}{d w_{kj}} \\
&= -2 \sum_{i=1}^n (y_i - \hat{y}_i ) \beta_{k} g' \left(w_{k0} + \sum_{j=1}^p w_{kj} x_{ij}\right) x_{ij},
\end{align*}
where the specific form depends on the activation function $g()$. But hopefully we choose a differentiable $g()$ where this isn't too complicated.

Note that both of these partial derivatives imply that the direction of our step depends on our residuals $y_i - \hat{y}_i$: this is a good thing! We want to make the residuals smaller! 

The reason that we call this special version of gradient descent ``backpropagation" is that we can think of these applications of the chain rule as stepping backward through our network, and passing our residuals/errors along as we go. As we add more layers to the network, we add more chain rules! But nothing gets too difficult. 

Recall that gradient descent lets:
$$
\theta^{t+1} = \theta^t - \rho \nabla L (\theta^{t}).
$$
So, the size of the step depends on the learning rate $\rho$, the size of the residuals $(y_i - \hat{y}_i)$, the magnitude of the gradient evaluated at $\theta^{t}$, and the actual inputs $x_{ij}$. This is a lot of things! Making sure that we take the right sized steps can be very finicky. 

\subsection{Stochastic Gradient Descent}

Instead of computing the loss function and the gradient over all $n$ training instances at once, we typically go instance-by-instance or batch-by-batch. This is related to \emph{online learning}, which you read about for HW2. A batch is just a subset of the $n$ training observations. The motivation is that we will speed up our learning about $\theta$ by updating $\theta^{t}$ more often. If our training set is huge, we don't want to bother processing a massive training set with a bad guess $\theta^{t}$. We should take advantage of the fact that we see a lot of errors in the first few training examples, and we should update $\theta^{t}$ to $\theta^{t+1}$ right away. 

This is stochastic because our gradient computed on a small number of training observations should be a good approximation of our overall gradient. But the exact step we take depends on the specific training observation! VERY STRANGELY: It turns out that stochastic gradient descent enforces its own form of approximately quadratic regularization. We will come back to this! So there is something really important about the relationship between stochastic gradient descent and overfitting, that wasn't necessarily the original motivation for SGD. 

It also turns out that the extra noise added with SGD can help us avoid local minima! That is cool too! We bounce around $\theta$-space and explore a bit more. 

If we are using SGD and we end up iterating through the entire training set 10 times, this means that we used 10 epochs. But, this might mean that we used $T=100$ updates of $\theta$, if our batch size was 1/10 of the training set. 

\subsection{Complications}

While (stochastic) gradient descent is a very general idea that can be used in many contexts, it is ultimately only a way to find an approximate minimizer of a loss function. There are a lot of complications.

We need to choose our step size and our initial guesses for the parameters. We also need to be worried about our initial starting conditions; as these can determine whether or not we get stuck in a local minimum. With bad initial weights (too large), we can have vanishing gradients, and we will think we have converged right away, without ever updating our initial weights. Or, with a ReLU activation function, it is the case that a certain unit is never ``turned on" for the training set given the initial weights, it will actually NEVER get turned on. It is a permanently dead Neuron. All of this is bad! Maybe I will put one of these on your HW. 

We also need to know how big of a batch we should use for SGD. 

We need to know how many iterations or epochs to train for. In theory, I guess we should go until the $\theta^t$ stop changing (convergence). But ... what if we have used up all of our computational resources and we have not yet converged? 

All of these things ACTUALLY affect our fit. Fitting a neural network can be so finicky in practice, unless you have a lot of resources and are an expert. This makes them a bit less usable or ``off-the-shelf" than some alternatives. 

We also need to decide on a model architecture before we start. How many hidden nodes do we want? How many hidden layers? What activation function should we use? Etc. 

\subsection{Overfitting and model complexity}

We know that a neural network model with $K$ hidden units in one layer and $p$ inputs needs to learn around $(p+1)*(K+1)$ parameters. If we add more hidden laters, this just gets bigger.

We know that, if we have $n$ training datapoints, a model with more than $n$ parameters should be able to memorize the training set. And we know that this is bad, because we will overfit, and will have poor generalization error. 

In modern deep neural networks, we often have more parameters than datapoints! So, how can we prevent overfitting?

There are several strategies that people were already doing:
\begin{itemize}
\item Add explicit regularization to our loss function! Like $\lambda$ times the L1 or L2 norm of our $w$s and our $\beta$s. This affects every step of our gradient descent calculation, and could make it more complicated. But it will keep the variance of our model down, as we know. 
\item Random dropout: A possibly simpler version of regularization. We can just randomly turn off some hidden nodes when we pass some training observations through the network. This forces other nodes to pick up the slack of these turned-off nodes. This means that a single node cannot be used to memorize a single training point---- because it has to perform multiple tasks. What a super cool idea!  
\item Early stopping via a validation set. Suppose that we want to encourage our weights to be small. We can start with small initial values. Then, every time we finish a batch or an epoch, we can check our error on a validation set. Presumably at first, this will be decreasing as we update our weights. If, at some point it starts increasing, we can assume that we started overfitting, and we can just stop training (even if we haven't converged). This is cool and should also work well! 
\end{itemize}
Note that any type of regularization adds to our training complications. How to we pick our penalty parameter? How do we pick how much dropout to have, or how MUCH the validation error needs to increase in order for us to think we should stop? There is already so much going on! This is not automated! 

Ok, but even with all of these strategies, don't we think that these massive models will overfit? And perform badly in terms of the bias-variance tradeoff? Especially when we don't have that many training observations? 
\begin{itemize}
\item Mostly, yes I do think this!
\item But empirically, we have seen a strange phenomenon called double descent.
\item It turns out it is not actually that strange or mysterious! Let's explore. 	
\end{itemize}


\subsection{Double descent, and its relation to stochastic gradient descent}

This is one of my favorite topics! See ISL 10.8 for a beautiful explanation. You will also explore this concept on your homework, and I may fill in these notes in more detail after Thursday's class. 

The story of double descent is shown in Figure~\ref{fig_dd}. 

\begin{figure}
\centering 
\includegraphics[width=0.6\textwidth]{442_lecs/dd.png}
\caption{A classic illustration of double descent.}	
\label{fig_dd}
\end{figure}





\subsection{Extensions of neural networks}

We have barely scratched the surface. There are so many cool topics we could cover. Such as multitask learning, auto-encoders, convolutional neural networks, or recurrent neural networks. Auto-encoders also relate to semi-supervised learning. We are not going to cover these! But hopefully you now have a little bit of foundational knowledge to understand these better in the future. And you could always do one of these for your final project!




