\section{Monday, April 14: Boosting}

Last time, we defined \emph{ensemble methods}. We then talked more specifically about bagging, which is a general-purpose technique that is designed to take a single model with low bias but high variance (e.g. deep decision tree) and reduce the variance through averaging. Today, we will talk about boosting, which is a general-purpose technique that is designed to take a single ``weak learner" with low variance but high bias and adaptively improve it until it becomes a ``strong learner". Today is another day where we will get to learn about how really key innovations came from both computer scientists and statisticians at around the same time, and then were combined to make something even better!

\subsection{AdaBoost (Freund and Schapire, 1996}

AdaBoost is an important early boosting algorithm! It is not covered in ISL, but is covered in depth in ESL!

\subsubsection{Algorithm}

The ideas for boosting were first born in theoretical computer science, and started with classification. We have $n$ datapoints, and our responses $y$ are either $-1$ or $1$: these represent two classes. 

The idea was to wonder: is it possible to take a weak learner or a base learner (any algorithm that performs slightly better than random guessing) and ``boost" it until it becomes an arbitrarily accurate ``strong" learner? It turns out that it is possible! The idea involves fitting the weak learner $T$ times during rounds $t=1,\ldots,T$. At each round, we re-weight the data points to focus on the ones that we misclassified previously. The pseudo-code is as follows.

\begin{itemize}
\item Start with training set $(x_1,y_1),\ldots,(x_n,y_n)$, where all weights are $w_1(i) = 1/n$ for $i=1,\ldots,n$.
\item For $t=1,\ldots,T$:
\begin{enumerate}
\item Take a sample of size $n$ from the empirical distribution on the dataset with weights $w_1,\ldots,w_n$.\footnote{In round 1, this is just a bootstrap sample. But this will change later!} If our weak learner allows the use of ``weights", we can also skip this step and go right to step 2. 
\item Train the weak learner on the subsampled or weighted dataset, $\hat{f}_t(x)$. Compute its training error, $e_t$, on the weighted or subsampled dataset. 
\item Let $\alpha_t = \frac{1}{2} \log\left( \frac{1- e_t}{e_t} \right)$. 
\item Update the weights! Let:
$$
\tilde{w}_{t+1}(i) = w_t(i) \exp \left( -\alpha_t y_i \hat{f}_t(x_i)\right)
$$
and then let $w_{t+1}(1), \ldots, w_{t+1}(n)$ be the $\tilde{w}_{t+1}(i)$ values but normalized to sum to $1$ so that we have a probability distribution over our training examples.  
\end{enumerate}
\item The final learner is:
$$
\hat{f}_{boost}(x) = \text{sign}\left( \sum_{t=1}^T \alpha_t \hat{f}_t(x) \right).
$$
\end{itemize}
That is it! That is AdaBoost! 

\subsubsection{Intuition}

We need to talk about exactly what is going on in each step. Even though boosting is now much more general than AdaBoost, AdaBoost is such an important early example that I think it is worth focusing on some intuition for AdaBoost before we go further.
\begin{itemize}
\item First let's discuss $\alpha_t$. This is the weight that each weak learner $\hat{f}_t(x)$ will end up getting towards the final prediction. This is based on the training error $e_t$ of each weak learner. The only thing we assume about $e_t$ is that our weak learner can do better than random guessing on our training set: thus, $e_t$ is between $0$ and $0.5$ for a binary classification task. On this domain, the weight $\alpha_t$ is large and positive if the training error is near 0, and is near 0 if the training error is near 0.5. So ... the classifiers that do well on their own training sets get more say in the final prediction! This seems reasonable!
\item Now let's talk about the weight updates $w_t$. Recall that we are assuming we wrote down and stored $y$ as $-1$ and $1$. And we are assuming that the weak learner $\hat{f}(x)$ outputs either $-1$ or $1$ for each datapoint. So, the term $y_i \hat{f}_t(x_i)$ in the weight update term will be positive whenever we correctly classified this point, and will be negative otherwise. 
\begin{itemize}
\item So, correctly classified points get $\tilde{w}_{t+1} = w_t(i) \exp (-\alpha_t)$. 
\begin{itemize}
\item If this weak learner was doing a good job overall, then $\alpha_t$ is big and $\exp (-\alpha_t)$ is near $0$. So, in this case, correctly classified points get small weights in the next round! We don't need to focus on them again! 
\item On the other hand, if this weak learner was doing a bad job overall, then $\alpha_t$ is near $0$ and $\tilde{w}_{t+1} \approx w_t$: we will essentially try again with current weights to see if we can improve. 
\end{itemize}
\item Incorrectly classified points get re-weighted as $\tilde{w}_{t+1} = w_t(i) \exp (\alpha_t)$. 
\begin{itemize}
\item If this weak learner was doing a good job overall, then $\alpha_t$ is big and we will increase the weight of this point a LOT. The idea is like ``wow, even for an overall really good learner, we missed this point. Now we need to go focus on it."
\item If this weak learner was doing a bad job overall, then $\alpha_t$ is near $0$ and $\tilde{w}_{t+1} \approx w_t$. So we never make large updates based on a weak learner that isn't even doing well! 
\end{itemize}
\end{itemize}
\item At the end, each weak learner gets to contribute a $+1$ or $-1$ vote to the classification. Its vote matters more if it had a big $\alpha_t$. 
\end{itemize}

\subsubsection{Practical considerations, or cool properties?}
\begin{itemize}
\item By design, AdaBoost definitely continually reduces training error. If each $\hat{f}_t(x)$ is a bit better than random guessing on the training set, the more of these that we combine the better that we do. Our training error should actually go to $0$ if $T$ is big enough. This property holds for binary classification where we start with anything better than random guessing: but might not be practically super interesting because this says nothing about test error. 
\item People were at first worried that AdaBoost would certainly overfit if $T$ is too big. There are error bounds, like the one you worked with on your midterm but based on VC dimension and not degrees of freedom, that confirm this. HOWEVER, in practice, it seemed like AdaBoost worked pretty well, even if $T$ was so big that we had reached $0$ training error.
\item What is going on?!?!?! Why aren't we overfitting at the point of $0$ training data? 
\item Even after AdaBoost has reached a point of $0$ training error, we can keep iterating and keep re-fitting with new weights. The consequence of this tends to be that we increase our margin of confidence for our classification: much like an SVM! We know from SVMs that increasing the margin can be GOOD for generalization error. So ... that's cool! 
\item AdaBoost can certainly overfit. But ... each of these individual hypotheses is weak, and we don't let ourselves make a prediction unless we are confident in it from a diverse set of learners. So ... it isn't so bad.
\end{itemize}


A note on statistics vs. computer science culture. As you know from Breiman, statistics literature often starts by assuming a data-generating mechanism for the data. On the other hand, a lot of computer science machine learning literature starts from a ``no-noise" setting, where we are really doing like pattern-recognition, not statistics. Boosting (like SVMs) was first studied by people in the no-noise setting! Overfitting is of course less of an issue here-- bias is way more of an issue! This colored some early bagging vs. boosting comparisons. It was found that bagging worked better in situations with noise: boosting works well in low-noise settings where overfitting is not really a concern: bias is the concern. 

Freund and Schapire certainly note that, while AdaBoost can work really well off-the-shelf with some nice properties, it is clearly dependent on the data and the weak learner. It can fail, and is particularly susceptible to noise. 

\subsubsection{Comparison to Bagging}

Now that we understand a bit more, we can compare AdaBoost to bagging!
\begin{itemize}
\item The idea of boosting is to start with a weak learner and improve it sequentially. The focus is on reducing bias by focusing on hard examples. 
\item The idea of bagging is to start with a learner that has high variance and reduce the variance through averaging. 
\item Both lose interpretability compared to a single model, but can have much higher accuracy.
\item Both are supposedly general purpose and off the shelf, but you might still need to do some tweaking or tuning!
\item Bagging can be trained in parallel; boosting cannot be because it is adaptive. For huge data applications, we might care about this!
\item Bagging might work better in high-noise situations. And in statistics, we always think there is noise!
\item Both have some beautiful theory attached that is beyond the scope of this class!
\end{itemize}

\subsection{Boosting is an additive model? (Friedman, Hastie, and Tibshirani, 2000)}

Boosting is far more general than just AdaBoost! It can be extended to multi-class classification and regression. In the process of extending it, people realized it has beautiful connections to other concepts in statistics! For example, you could have each weak learner $\hat{f]_t(x)$ in AdaBoost return a probability prediction, not a $-1$ or $1$ prediction. This allows more information about confidence and size of error when computing future weights. This makes sense! This then leads to a view (pointed out by Breiman, among others) that we can see AdaBoost as doing gradient descent on an exponential loss. 

We don't have time to do justice to all of these connections, but let's talk about one really important paper: Friedman, Hastie, and Tibshirani  (2000). They connect boosting to additive logistic regression. These insights helped with computational efficiency of boosting, let us study its theoretical properties with a wider array of tools, and also led to further extensions! This paper is summarized really nicely in ESL Chapter 10! 

The key insight is that AdaBoost can be seen as an iterative forward stage-wise algorithm for fitting an additive logistic regression model, that optimizes a particular loss function. This insights helps them see that AdaBoost can be both simplified but also greatly generalized! 

Recall that, in an additive model:
$$
\hat{f}(x) = \sum_{j=1}^p \hat{f}_j(x_j).
$$
This could either be a linear regression model, or could be a logistic regression model, in which case $\hat{f}(x)$ is modeling the log odds of belonging to a certain class. This model does not allow predictors $x_j$ and $x_k$ to interact, unless we pre-specify the interaction as its own feature. This was a main limitation. But, a key connection to boosting is remembering the back-fitting algorithm that we used to fit this model. We went term-by-term and we iteratively fit a relatively simple model to the current residuals from the remaining terms. That iterative process sounds a bit like boosting, no? 

To further see the connections to boosting, we can make the additive model less additive. Let $b(x)$ be some sort of basis function that is allowed to use ALL of the predictors $x$, as well as some other parameters $\gamma$. Then, we could let:
$$
\hat{f}(x) = \sum_{m=1}^M \beta_m b(x, \gamma_m). 
$$
We talked about how a neural network can be written in this way! Where $b()$ is the activation function, $\gamma_m$ are the weights in the first layer, and $\beta_M$ are the weights in the activation layer. This is a really complicated basis expansion, but can nonetheless be seen as a basis expansion. We talked about how the people behind Projection Pursuit proposed a model like this, but (at the time), there was not an efficient and feasible way to fit such a thing! 

AdaBoost provides a lens through which to imagine fitting this model, that looks a bit different than neural networks. Each $b_m()$ could be a 2-level tree, where the parameters $\gamma_m$ that we need to learn are the split variables and the split points. By fitting the tree to the residuals from previous iterations, we are focusing on points that we previously got wrong. This starts to look a lot like boosting: iteratively fitting trees to previous residuals!

It turns out that AdaBoost exactly fits this model, but it does so in a way that minimizes an exponential loss function for classification. You could change this to the binomial negative log likelihood to make it more closely resemble standard logistic regression. You could also change this to any other loss function for regression. Boosting fits an additive model with weak learners as basis functions, and it minimizes a loss function using iterative gradient descent! 

The idea of a forward stage-wise model is that we iteratively add new basis functions to the expansion without allowing ourselves to change the ones that have already been added. This is what makes it boosting. We always have a current model, and we are always looking to improve whatever is left over. We add a new basis term to the residuals! 

Forward stage-wise boosting is a very greedy strategy. We always take the step that most reduces the loss function right now. Recall that gradient descent updates initial guesses for parameters by moving them in the direction that will most reduce the lost function right now. Thus, boosting is really just gradient descent- where we take our steps not necessarily in real-valued parameter space, but rather in function space (we can take a step in the direction of a tree).

We are not going to cover gradient descent in detail! But you should know that it exists, and how to use it! 
