\newpage
\section{Thursday, April 17: Explainability vs. Interpretability!}

We are going to spend some time today covering the boosting content that we missed. We are also going to have a short discussion of the ``two cultures" reading. Thus, we won't have time to cover too much new content today. 

For the last few classes, we have thrown interpretation to the wind. We have talked about selecting models based on predictive accuracy, and we have talked about ensembles, which are certainly hard to interpret. We have been imagining for the last few classes that our primary goal is predictive accuracy.

Today, let's assume that our primary goal is to learn something about the world. We don't need to make predictions for a new, unseen dataset. We need to understand the data that is in front of us! There are actually two ways that we could accomplish this goal. 

\begin{itemize}
\item We can fit an \textbf{interpretable} model, otherwise known as a \emph{glass box}. Examples include simple or sparse linear models, or small decision trees. With these models, we know exactly what variables are playing a role in our predictions, which presumably helps us understand the world.
\item We can fit a \emph{black-box} model, but then \textbf{explain} this model after the fact using a separate algorithm. This is the huge and growing field known as explainable AI. The only example we have mentioned so far of an explainability technique is permutation importance for random forests (this was also in your Breiman reading). But we haven't yet done this topic justice!
\end{itemize}

One theme of today is that this dichotomy is kind of useful, but that its also really tricky do define these things. What is interpretable depends on your audience! And what you are trying to convey!

Today, I want to just briefly touch on possible explainability techniques for determining important variables. They are both very simple, and pretty general-purpose with a lot of room for tweaks. There are many versions of these out there: I am presenting really simple versions. We will use this just to have a language for discussing interpretability vs. explainability! 

\subsection{Permutation importance (no retraining)}

The idea is to figure out what variables are most contributing to your predictions in a given black-box model.

\begin{itemize}
\item Fit a model $\hat{f}()$ to predict $y$ using a training set. Measure the loss on a test set: i.e. compare $\hat{f}(x^{test})$ and $y^{test}$.
\item For variables $j=1,\ldots,p$:
\begin{itemize}
\item Randomly permute the values of variable $j$ in the test set to get $\tilde{x}^{test}$.
\item Measure the loss between $\hat{f}(\tilde{x}^{test})$ and $y^{test}$. 
\item The increase in loss from permuting the $j$th variable is the importance of the $j$th variable. 
\end{itemize}
\end{itemize}

This idea is really simple, and is effective in telling us which variables are contributing to our predictions. But, it has a lot of problems if we try to interpret it as a measure of the association between variable $j$ and $y$.

Imagine that $x_j$ and $x_k$ are extremely highly correlated, and both associated with $y$. But imagine that our model decides to only use $x_k$ for its predictions. Well, then according to permutation importance, $x_j$ is not important at all. But that doesn't mean it is not associated with $y$- it is just being masked by $x_k$. 

This gets at a particular definition of importance: a variable is not important if we can make good predictions without this variable. The thing that it does not take into account is the fact that maybe, without this covariate, we would have fit a different model! This is something that the next method tries to address. 

Note that glass-box models have this issue too though: this just means that attribution is really hard. A linear regression cannot always tell which of two correlated predictors is important! So, this isn't a unique failing of permutation importance; just something to point out!

\subsection{Leave one out importance (with retraining)}

This one is actually even more strange! The idea is subtly different, and this is much more computationally expensive than the idea above.

\begin{itemize}
\item Fit a model $\hat{f}()$ to predict $y$ using a training set. Measure the loss on a test set: i.e. compare $\hat{f}(x^{test})$ and $y^{test}$.
\item For variables $j=1,\ldots,p$:
\begin{itemize}
\item Leave variable $j$ out of the training set to get $x^{train}_{-j}$.
\item Refit the model on the training set. Now you have $\hat{f}_{-j}()$. 
\item Measure the loss between $\hat{{f}}_{-j}({x}^{test}_{-j})$ and $y^{test}$. 
\item The increase in loss from removing the $j$th variable is the importance of the $j$th variable. 
\end{itemize}
\end{itemize}

In some ways, this feels more thorough to me. But in the case where $x_j$ and $x_k$ are extremely highly correlated, and both associated with $y$, I think we get even worse results. In the previous example, at least $x_k$ would have popped up as important. Here, when $j$ is left out, $k$ can absorb its effect. But when $k$ is left out, $j$ can absorb its effect. So we might end up reporting neither of these as important! This is not desirable! 

This again is something that can happen in interpretable models too! If we include $x_j$ and $x_k$ both in a regression in this setting, the variance inflation factors that come from their correlation could lead us to report neither as significant. 

\subsection{Model distillation (student/teacher methods)}

The idea of model distillation is to first train a black box that achieves high classification accuracy. This is the ``teacher" model. The next idea is to take the predicted values from this model, which may be probabilities instead of hard classifications, and try to predict these using the covariates using a simple, glass-box like a tree. The idea is that maybe a tree would not originally have done very well on this data. But we have now distilled the response variable into pieces that can be explained by the covariates, and we are giving the tree hints about which observations we are more or less confident about. So now all of the sudden, the tree can do better at explaining these predictions than it could have done at predicting the original classes. Huh! Cool! I would like to learn more about this some day!

\subsection{Other explainability methods?}

There are many other methods that Lucy or our guest speaker next week might tell us about! And I will try to post some additional readings! This could probably be an entire class, so it was hard to know what to focus on!

\subsection{Do we want interpretability? Or is explainability enough?}

\subsubsection{Some arguments for interpretability}

A quote that I love from the Sankaran reading posted on GLOW is that an interpretable model supplements human decision-making, rather than replacing it. And this is how we want to be using our models! Cynthia Rudin, in reading that you will do for HW7, implies that explanations are not always true to what the model is actually doing, and can get us into trouble. If we really truly want transparency, then interpretable models might be the way to go. 

\subsubsection{Some arguments for explainability}

Breiman strongly makes the point that models that are simple enough to be interpretable are often less accurate. He says that this is bad, because we only want to learn about the world from a model that is accurate enough to explain the world. I actually have some sympathy for this viewpoint: can we really learn something trustworthy about the world from a model with really low predictive accuracy? I am not sure! The economists would say yes!

A point that I take more seriously is the point about stability. We love trees because they are supposedly interpretable. But, we also know that a tiny perturbation to a training set could cause a tree to look totally different and have totally different variables at the topmost splits. Should we really be interpreting something so unstable? With this in mind, fitting many trees to bagged samples and recording which variables get selected over and over again suddenly seems a lot more appealing as a measure of variable importance.

Overall, we like that we can see into glass boxes. But, if the glass boxes don't work well or are not stable, why do we care? I think these are reasonable points!

\subsubsection{More coming!}

The reading you do for HW7, as well as the guest lecture next Thursday, should help reinforce these concepts!



